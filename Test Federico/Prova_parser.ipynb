{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31be67a6-630c-482e-a80f-082b3d46fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import bs4 as bs\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668e21c6-b808-4f68-a141-5331c74503c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from re import search\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3796eebd-c09c-400f-97d7-f00de8ac9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_place(page):\n",
    "    \n",
    "    # Read local files\n",
    "    with open(page,encoding=\"utf-8\") as f:\n",
    "        soup = bs.BeautifulSoup(f)\n",
    "    \n",
    "    placeName = soup.find_all('h1', {'class':'DDPage__header-title'})[0].contents[0]\n",
    "    \n",
    "    placeTags = list()\n",
    "    for tags in soup.find_all('a', {'class':'itemTags__link js-item-tags-link'}):\n",
    "        wordlen=len(tags.text)-2\n",
    "        tag = tags.text[1:wordlen]\n",
    "        placeTags.append(str(tag))\n",
    "    \n",
    "    been = soup.find_all('div', {'class':'col-xs-4X js-submit-wrap js-been-to-top-wrap action-btn-col hidden-print'})[0]\n",
    "    num_been = been.get_text().split()\n",
    "    numPeopleVisited = int(num_been[2])\n",
    "    if numPeopleVisited==0:\n",
    "        numPeopleVisited = ''\n",
    "    \n",
    "    want = soup.find_all('div', {'class':'col-xs-4X js-submit-wrap js-like-top-wrap action-btn-col hidden-print'})[0]\n",
    "    num_want = want.get_text().split()\n",
    "    numPeopleWant = int(num_want[3])\n",
    "    if numPeopleWant==0:\n",
    "        numPeopleWant = ''\n",
    "    \n",
    "    description = soup.find('div', class_='DDP__body-copy')\n",
    "    allowlist = ['p', 'span', 'a', 'i']\n",
    "    text_elements = [t for t in description.find_all(text=True) if t.parent.name in allowlist]\n",
    "    placeDesc = str(' '.join(text_elements))\n",
    "    placeDesc = placeDesc.replace(u'\\xa0',u' ')\n",
    "    \n",
    "    \n",
    "    placeShortDesc = soup.find_all('h3', {'class':'DDPage__header-dek'})[0].contents[0]\n",
    "    placeShortDesc = placeShortDesc.replace(u'\\xa0',u' ')\n",
    "    placeShortDesc = str(placeShortDesc)\n",
    "    \n",
    "    placeNearby=list()\n",
    "    for places in soup.find_all('div', {'class':'DDPageSiderailRecirc__item-title'}):\n",
    "        placeNearby.append(str(places.text))\n",
    "    if len(placeNearby) == 0:\n",
    "        placeNearby = ''\n",
    "    \n",
    "    \n",
    "    placeRaw= soup.find_all('address', class_='DDPageSiderail__address')[0]\n",
    "    place = placeRaw.find_all('div')[0].contents[0:5:2]\n",
    "    place = \" \".join(place)\n",
    "    placeAddress = place.replace('\\n', '')\n",
    "    \n",
    "    \n",
    "    coordinates = soup.find_all('div', class_='DDPageSiderail__coordinates')[0]\n",
    "    coordinates = coordinates.get_text().split()\n",
    "    Alt = coordinates[0]\n",
    "    Altlen = len(Alt)\n",
    "    placeAlt = float(Alt[0:Altlen-1])\n",
    "    placeLong = float(coordinates[1])\n",
    "    \n",
    "\n",
    "    editors = soup.find_all('li', {'class':'DDPContributorsList__item'})\n",
    "    if len(editors)==0:\n",
    "        #placeEditors = soup.find_all('div', {'class':'DDPContributorsList'})[1].get_text().split()\n",
    "        #TODO: check the line below\n",
    "        listEditor = soup.find_all('div', {'class':'DDPContributorsList'})\n",
    "        if len(listEditor) == 0:\n",
    "            placeEditors=[\"\"]\n",
    "        else:\n",
    "            placeEditors = listEditor[0].get_text().split()\n",
    "    else:\n",
    "        placeEditors = list()\n",
    "        for place in editors:\n",
    "            names = place.find('span').getText()\n",
    "            placeEditors.append(names)\n",
    "    \n",
    "    \n",
    "    date_time = soup.find_all('div', {'class':'DDPContributor__name'})[0].get_text()\n",
    "    placePubDate = datetime.strptime(date_time, '%B %d, %Y')\n",
    "    \n",
    "    \n",
    "    titles = soup.find_all('h3', class_='Card__heading --content-card-v2-title js-title-content')  \n",
    "    placeRelatedPlaces = list()\n",
    "    for title in titles:\n",
    "        big_check = title.parent.parent.parent.parent.parent.parent\n",
    "        check = big_check.find('div', class_=\"CardRecircSection__title\").get_text()\n",
    "        if check == 'Related Places':\n",
    "            placeRelatedPlaces.append(str(title.get_text().strip()))\n",
    "    \n",
    "    placeRelatedLists = list()\n",
    "    for title in titles:\n",
    "        big_check = title.parent.parent.parent.parent.parent.parent\n",
    "        check = big_check.find('div', class_=\"CardRecircSection__title\").get_text()\n",
    "        if search(\"Appears in\", check):\n",
    "            placeRelatedLists.append(str(title.get_text().strip()))\n",
    "    if len(placeRelatedLists)==0:\n",
    "        placeRelatedLists.append('')\n",
    "    \n",
    "    find_url = soup.find('link', {\"rel\": \"canonical\"})\n",
    "    placeURL = find_url['href']\n",
    "    \n",
    "    #print(\"placeName: \"+str(len(placeName)))\n",
    "    #print(\"placetags \"+str(len(placeTags)))\n",
    "    #print(\"address \"+str(len(placeAddress)))\n",
    "    #print(\"editors \"+str(len(placeEditors)))\n",
    "    #print(\"relatedplaces \"+str(len(placeRelatedPlaces)))\n",
    "    #print(\"relatedlists \"+str(len(placeRelatedLists)))\n",
    "\n",
    "    \n",
    "    return {'placeName': placeName,\n",
    "            'placeTags': str(placeTags),\n",
    "            'numPeopleVisited': numPeopleVisited,\n",
    "            'numPeopleWant': numPeopleWant,\n",
    "            'placeDesc': placeDesc,\n",
    "            'placeShortDesc':placeShortDesc,\n",
    "            'placeNearby':str(placeNearby),\n",
    "            'placeAddress': placeAddress,\n",
    "            'placeAlt': placeAlt,\n",
    "            'placeLong': placeLong,\n",
    "            'placeEditors': str(placeEditors),\n",
    "            'placePubDate': placePubDate,\n",
    "            'placeRelatedPlaces': str(placeRelatedPlaces),\n",
    "            'placeRelatedLists': str(placeRelatedLists),\n",
    "            'placeURL': placeURL}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88878e73-f876-4351-8975-c4768dfa0d2e",
   "metadata": {},
   "source": [
    "reference:\\\n",
    "https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba0ce34-0d42-4c6c-9ee2-e335cd7234cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_maker(pages:list ,dir=\"downloads\"):\n",
    "    raws = []\n",
    "    \n",
    "    for page in pages:\n",
    "        df = pd.DataFrame(extract_single_place(f\"{dir}/{page}\"), index=[0])\n",
    "        raws.append(df)\n",
    "    return pd.concat(raws)\n",
    "\n",
    "def parallel_table(dir=\"downloads\"):\n",
    "    n_cores = cpu_count()\n",
    "    files = os.listdir(dir)\n",
    "    files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "    chunks = np.split(np.array(files), n_cores)\n",
    "    \n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(table_maker, chunks))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c6f04-d689-4cae-ae3b-d9e72968d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_pages = parallel_table(\"./../html/downloads/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111b22c-7c0f-4230-bd16-e344c6b8241c",
   "metadata": {},
   "source": [
    "## Only this method below seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ece9c8a-8f78-4779-a850-5663783498b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_maker(pages:list ,dir=\"downloads\"):\n",
    "    index = extract_single_place(f\"{dir}/{pages[0]}\") # create index\n",
    "    with open(\"result.tsv\",\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\t\".join(list(map(str,index.keys()))) + \"\\n\")\n",
    "    for page in pages:\n",
    "        cols = extract_single_place(f\"{dir}/{page}\")\n",
    "        with open(\"result.tsv\",\"a\",encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\t\".join(list(map(str,cols.values()))) + \"\\n\")\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63bc530-b8a1-497c-9c18-a21420f09636",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4404/2504070582.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".ipynb_checkpoints\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtable_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./../html/downloads\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4404/1733103769.py\u001b[0m in \u001b[0;36mtable_maker\u001b[1;34m(pages, dir)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_single_place\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{dir}/{page}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"result.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4404/2963235906.py\u001b[0m in \u001b[0;36mextract_single_place\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mdate_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'DDPContributor__name'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[0mplacePubDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%B %d, %Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"./../html/downloads/\")\n",
    "files.remove(\".ipynb_checkpoints\")\n",
    "\n",
    "table_maker(files,\"./../html/downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42bf37-a9f4-44ac-8962-3c1f28c81cc7",
   "metadata": {},
   "source": [
    "### Debugging di extract_single_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91f813-5629-449e-9169-e3dcfa54c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "files = os.listdir(\"downloads/\")\n",
    "files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "for file in files:\n",
    "    try:\n",
    "        extract_single_place(f\"downloads/{file}\")\n",
    "    except Exception as e:\n",
    "        print(file,\"-->\" ,traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
