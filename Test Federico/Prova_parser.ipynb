{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31be67a6-630c-482e-a80f-082b3d46fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import bs4 as bs\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668e21c6-b808-4f68-a141-5331c74503c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from re import search\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3796eebd-c09c-400f-97d7-f00de8ac9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_place(page):\n",
    "    \"\"\"\n",
    "    Extract the requested features from a local html page\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read local files\n",
    "    with open(page,encoding=\"utf-8\") as f:\n",
    "        soup = bs.BeautifulSoup(f)\n",
    "    \n",
    "    placeName = soup.find_all('h1', {'class':'DDPage__header-title'})[0].contents[0]\n",
    "    \n",
    "    placeTags = list()\n",
    "    for tags in soup.find_all('a', {'class':'itemTags__link js-item-tags-link'}):\n",
    "        wordlen=len(tags.text)-2\n",
    "        tag = tags.text[1:wordlen]\n",
    "        placeTags.append(str(tag))\n",
    "    \n",
    "    been = soup.find_all('div', {'class':'col-xs-4X js-submit-wrap js-been-to-top-wrap action-btn-col hidden-print'})[0]\n",
    "    num_been = been.get_text().split()\n",
    "    numPeopleVisited = int(num_been[2])\n",
    "    if numPeopleVisited==0:\n",
    "        numPeopleVisited = ''\n",
    "    \n",
    "    want = soup.find_all('div', {'class':'col-xs-4X js-submit-wrap js-like-top-wrap action-btn-col hidden-print'})[0]\n",
    "    num_want = want.get_text().split()\n",
    "    numPeopleWant = int(num_want[3])\n",
    "    if numPeopleWant==0:\n",
    "        numPeopleWant = ''\n",
    "    \n",
    "    description = soup.find('div', class_='DDP__body-copy')\n",
    "    allowlist = ['p', 'span', 'a', 'i']\n",
    "    text_elements = [t for t in description.find_all(text=True) if t.parent.name in allowlist]\n",
    "    placeDesc = str(' '.join(text_elements))\n",
    "    placeDesc = placeDesc.replace(u'\\xa0',u' ')\n",
    "    \n",
    "    \n",
    "    placeShortDesc = soup.find_all('h3', {'class':'DDPage__header-dek'})[0].contents[0]\n",
    "    placeShortDesc = placeShortDesc.replace(u'\\xa0',u' ')\n",
    "    placeShortDesc = str(placeShortDesc)\n",
    "    \n",
    "    placeNearby=list()\n",
    "    for places in soup.find_all('div', {'class':'DDPageSiderailRecirc__item-title'}):\n",
    "        placeNearby.append(str(places.text))\n",
    "    if len(placeNearby) == 0:\n",
    "        placeNearby = ''\n",
    "    \n",
    "    \n",
    "    placeRaw= soup.find_all('address', class_='DDPageSiderail__address')[0]\n",
    "    place = placeRaw.find_all('div')[0].contents[0:5:2]\n",
    "    place = \" \".join(place)\n",
    "    placeAddress = place.replace('\\n', '')\n",
    "    \n",
    "    \n",
    "    coordinates = soup.find_all('div', class_='DDPageSiderail__coordinates')[0]\n",
    "    coordinates = coordinates.get_text().split()\n",
    "    Alt = coordinates[0]\n",
    "    Altlen = len(Alt)\n",
    "    placeAlt = float(Alt[0:Altlen-1])\n",
    "    placeLong = float(coordinates[1])\n",
    "    \n",
    "\n",
    "    editors = soup.find_all('li', {'class':'DDPContributorsList__item'})\n",
    "    if len(editors)==0:\n",
    "        #placeEditors = soup.find_all('div', {'class':'DDPContributorsList'})[1].get_text().split()\n",
    "        #TODO: check the line below\n",
    "        listEditor = soup.find_all('div', {'class':'DDPContributorsList'})\n",
    "        if len(listEditor) == 0:\n",
    "            placeEditors=[\"\"]\n",
    "        else:\n",
    "            placeEditors = listEditor[0].get_text().split()\n",
    "    else:\n",
    "        placeEditors = list()\n",
    "        for place in editors:\n",
    "            names = place.find('span').getText()\n",
    "            placeEditors.append(names)\n",
    "    \n",
    "    \n",
    "    date_time = soup.find_all('div', {'class':'DDPContributor__name'})\n",
    "    if len(date_time) >0:\n",
    "        time = date_time[0].get_text()\n",
    "        placePubDate = datetime.strptime(time, '%B %d, %Y')\n",
    "    else:\n",
    "        placePubDate = \"\"\n",
    "    \n",
    "    \n",
    "    titles = soup.find_all('h3', class_='Card__heading --content-card-v2-title js-title-content')  \n",
    "    placeRelatedPlaces = list()\n",
    "    for title in titles:\n",
    "        big_check = title.parent.parent.parent.parent.parent.parent\n",
    "        check = big_check.find('div', class_=\"CardRecircSection__title\").get_text()\n",
    "        if check == 'Related Places':\n",
    "            placeRelatedPlaces.append(str(title.get_text().strip()))\n",
    "    \n",
    "    placeRelatedLists = list()\n",
    "    for title in titles:\n",
    "        big_check = title.parent.parent.parent.parent.parent.parent\n",
    "        check = big_check.find('div', class_=\"CardRecircSection__title\").get_text()\n",
    "        if search(\"Appears in\", check):\n",
    "            placeRelatedLists.append(str(title.get_text().strip()))\n",
    "    if len(placeRelatedLists)==0:\n",
    "        placeRelatedLists.append('')\n",
    "    \n",
    "    find_url = soup.find('link', {\"rel\": \"canonical\"})\n",
    "    placeURL = find_url['href']\n",
    "    \n",
    "    #print(\"placeName: \"+str(len(placeName)))\n",
    "    #print(\"placetags \"+str(len(placeTags)))\n",
    "    #print(\"address \"+str(len(placeAddress)))\n",
    "    #print(\"editors \"+str(len(placeEditors)))\n",
    "    #print(\"relatedplaces \"+str(len(placeRelatedPlaces)))\n",
    "    #print(\"relatedlists \"+str(len(placeRelatedLists)))\n",
    "\n",
    "    \n",
    "    return {'placeName': placeName,\n",
    "            'placeTags': str(placeTags),\n",
    "            'numPeopleVisited': numPeopleVisited,\n",
    "            'numPeopleWant': numPeopleWant,\n",
    "            'placeDesc': placeDesc,\n",
    "            'placeShortDesc':placeShortDesc,\n",
    "            'placeNearby':str(placeNearby),\n",
    "            'placeAddress': placeAddress,\n",
    "            'placeAlt': placeAlt,\n",
    "            'placeLong': placeLong,\n",
    "            'placeEditors': str(placeEditors),\n",
    "            'placePubDate': placePubDate,\n",
    "            'placeRelatedPlaces': str(placeRelatedPlaces),\n",
    "            'placeRelatedLists': str(placeRelatedLists),\n",
    "            'placeURL': placeURL}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a5152-46b8-43b2-b40d-620ea707face",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"../../downloads/\"\n",
    "    \n",
    "res = [] \n",
    "files = os.listdir(dir)\n",
    "files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "\n",
    "# create first row of the file (with the index)\n",
    "pd.DataFrame(extract_single_place(f\"{dir}/{files[0]}\"),index=[0]).to_csv(\"res.tsv\",index=None,sep=\"\\t\")\n",
    "\n",
    "for file in files[1:]:\n",
    "    df = pd.DataFrame(extract_single_place(f\"{dir}/{file}\"),index=[0])  # append every row to the file creted before\n",
    "    if len(df)>1:\n",
    "         \n",
    "    df.to_csv(\"res.tsv\",index=None,mode=\"a\",sep=\"\\t\",header=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2f0ab-53ba-4469-9f45-a2783b4abd26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### END OF QUESTION 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88878e73-f876-4351-8975-c4768dfa0d2e",
   "metadata": {},
   "source": [
    "reference:\\\n",
    "https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba0ce34-0d42-4c6c-9ee2-e335cd7234cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_maker(pages:list ,dir=\"../../downloads/\"):\n",
    "    raws = []\n",
    "    \n",
    "    for page in pages:\n",
    "        d = extract_single_place(f\"{dir}/{page}\")\n",
    "        raws.append(d)\n",
    "    return raws\n",
    "\n",
    "def parallel_table(dir=\"downloads\"):\n",
    "    files = os.listdir(dir)\n",
    "    files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "    chunks = np.split(np.array(files), n_cores)\n",
    "    \n",
    "    with Pool() as p:\n",
    "        l = p.map(table_maker, files)\n",
    "\n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e778b-28fa-4874-ba28-934da5be216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"../../downloads/\"\n",
    "    \n",
    "res = [] \n",
    "files = os.listdir(dir)\n",
    "files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "\n",
    "pd.DataFrame(extract_single_place(f\"{dir}/{files[0]}\"),index=[0]).to_csv(\"res.tsv\",index=None,sep=\"\\t\")\n",
    "for file in files[1:]:\n",
    "    pd.DataFrame(extract_single_place(f\"{dir}/{file}\"),index=[0]).to_csv(\"res.tsv\",index=None,mode=\"a\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c6f04-d689-4cae-ae3b-d9e72968d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res =pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ce3b3-27dc-46a1-9785-7919ccb082a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111b22c-7c0f-4230-bd16-e344c6b8241c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Only this method below seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ece9c8a-8f78-4779-a850-5663783498b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_maker(pages:list ,dir=\"downloads\"):\n",
    "    index = extract_single_place(f\"{dir}/{pages[0]}\") # create index\n",
    "    with open(\"result.tsv\",\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\t\".join(list(map(str,index.keys()))) + \"\\n\")\n",
    "    for page in pages:\n",
    "        cols = extract_single_place(f\"{dir}/{page}\")\n",
    "        with open(\"result.tsv\",\"a\",encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\t\".join(list(map(str,cols.values()))) + \"\\n\")\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f63bc530-b8a1-497c-9c18-a21420f09636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"../../downloads/\")\n",
    "files.remove(\".ipynb_checkpoints\")\n",
    "\n",
    "table_maker(files,\"../../downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e6569-103b-45de-8e9d-ed670c2196be",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../../downloads/\")\n",
    "files.remove(\".ipynb_checkpoints\")\n",
    "\n",
    "res = table_maker(files,\"../../downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df744d8-4a21-4fa3-ac04-8a2816eed204",
   "metadata": {},
   "source": [
    "#### Done with pandas to reduce compatibility errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb6e34-fb3d-4bd8-829e-2bee721cce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"../../downloads/\"\n",
    "    \n",
    "res = [] \n",
    "files = os.listdir(dir)\n",
    "files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "\n",
    "pd.DataFrame(extract_single_place(f\"{dir}/{files[0]}\"),index=[0]).to_csv(\"res.tsv\",index=None,sep=\"\\t\")\n",
    "for file in files[1:]:\n",
    "    df = pd.DataFrame(extract_single_place(f\"{dir}/{file}\"),index=[0])\n",
    "    if len(df)>1:\n",
    "         \n",
    "    df.to_csv(\"res.tsv\",index=None,mode=\"a\",sep=\"\\t\",header=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42bf37-a9f4-44ac-8962-3c1f28c81cc7",
   "metadata": {},
   "source": [
    "### Debugging di extract_single_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91f813-5629-449e-9169-e3dcfa54c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "files = os.listdir(\"downloads/\")\n",
    "files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "for file in files:\n",
    "    try:\n",
    "        extract_single_place(f\"downloads/{file}\")\n",
    "    except Exception as e:\n",
    "        print(file,\"-->\" ,traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565930d4-7518-4ba5-a734-9ef5bf3b384b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1808c8-196f-4051-9463-881b7f33ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d36eae-d823-4dd6-af52-e4c8e816645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f5f995-916f-481e-b026-f9ca1e6ee007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner_2(description):\n",
    "    \"\"\"\n",
    "    stemming function\n",
    "    \"\"\"\n",
    "    if type(description) != str:\n",
    "        return None\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    punct =  set(string.punctuation)\n",
    "    punct.add(\"“\")\n",
    "    punct.add(\"”\")\n",
    "    punct.add(\"’\")\n",
    "\n",
    "    filtered_sentence = []\n",
    "    word_tokens = word_tokenize(description)\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words :\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    stemmed_desc = []\n",
    "    for w in filtered_sentence:\n",
    "        x = snow_stemmer.stem(w)\n",
    "        stemmed_desc.append(x)\n",
    "\n",
    "    filtered_desc = []\n",
    "    for s in stemmed_desc:\n",
    "        if s not in punct:\n",
    "            filtered_desc.append(s)\n",
    "\n",
    "    return filtered_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01b879d-6112-4da9-b27d-b52470fc77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_csv(\"res.tsv\", delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4570501-5dfe-43ad-8dce-aad187b8d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"cleanedDesc\"] = df_res.placeDesc.apply(text_cleaner_2)\n",
    "df_res[\"cleanedShortDesc\"] = df_res.placeShortDesc.apply(text_cleaner_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2523d997-9195-4903-9cd9-647ca4931bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the set of words contained in all the documents\n",
    "s = set()\n",
    "for desc in df_res.cleanedDesc:\n",
    "    if desc is not None:\n",
    "        for word in desc:\n",
    "            s.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da51d2a-aaf1-41ba-ae02-cde725755e5b",
   "metadata": {},
   "source": [
    "Creating the _vocabulary_ file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41e597f0-e3fa-4247-b7fa-2d08776258f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a number to each word in the set\n",
    "d, i = {}, 0\n",
    "for w in s:\n",
    "    d[w] = i\n",
    "    i += 1\n",
    "with open(\"vocabulary.json\", \"w\") as f:\n",
    "    f.write(json.dumps(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a46f6c9-30dd-4706-a759-a9c399ce6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.json\", \"r\") as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "reverse_index = {}\n",
    "for w in s:\n",
    "    filtered = df_res.cleanedDesc.apply(lambda x: x != None and w in x)   # filter the documents that cointain the word\n",
    "    docs = df_res[filtered].index.tolist()                                # save the index of the filtered files\n",
    "    reverse_index[vocabulary[w]] = docs\n",
    "\n",
    "with open(\"reverse_index.json\", \"w\") as f:                                # save the index in a json file\n",
    "    f.write(json.dumps(reverse_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee75de-1be0-426c-af9c-8ec43a87141e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fa8132-776b-49ca-8cb8-e7ce1b615af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_function(query, voc=\"vocabulary.json\", reverse_index=\"reverse_index.json\", df_name=\"res.tsv\"):\n",
    "    df = pd.read_csv(df_name, delimiter = '\\t')\n",
    "    with open(reverse_index, \"r\") as f:\n",
    "        reverse_index = json.load(f)\n",
    "    with open(voc, \"r\") as f:\n",
    "        vocabulary = json.load(f)\n",
    "    \n",
    "    idx = vocabulary[query.split()[-1]]                       # find the id of the first word\n",
    "    s = set(reverse_index[str(idx)])\n",
    "    for w in query.split()[:-1]:\n",
    "        idx = vocabulary[w]                                  # find the id for the remainig words\n",
    "        s.intersection(set(reverse_index[str(idx)]))  # perform the intersection on the sets of documents\n",
    "    return df.iloc[list(s)][[\"placeName\",\"placeDesc\",\"placeURL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07725021-0bb0-46ed-9c8b-9a61fa5b8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"system\"\n",
    "\n",
    "result = query_function(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6c83ff-6446-499d-91a7-742d1ee6021e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Snow Hole</td>\n",
       "      <td>In a rare geophysical phenomenon, snow  and ic...</td>\n",
       "      <td>https://www.atlasobscura.com/places/the-snow-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>Webster Place</td>\n",
       "      <td>Brooklyn has an abundance of  spectacular arch...</td>\n",
       "      <td>https://www.atlasobscura.com/places/webster-place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Eisbachwelle</td>\n",
       "      <td>When you think of wave  surfing you probably h...</td>\n",
       "      <td>https://www.atlasobscura.com/places/eisbachwelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>Rothschild Patent Model Collection</td>\n",
       "      <td>From 1790 to 1880 the  United States  required...</td>\n",
       "      <td>https://www.atlasobscura.com/places/rothschild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>Atlantic Avenue Tunnel</td>\n",
       "      <td>UPDATE: Currently closed by New  York departme...</td>\n",
       "      <td>https://www.atlasobscura.com/places/atlantic-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               placeName  \\\n",
       "1                          The Snow Hole   \n",
       "2049                      Webster Place    \n",
       "8                           Eisbachwelle   \n",
       "4108  Rothschild Patent Model Collection   \n",
       "2062              Atlantic Avenue Tunnel   \n",
       "\n",
       "                                              placeDesc  \\\n",
       "1     In a rare geophysical phenomenon, snow  and ic...   \n",
       "2049  Brooklyn has an abundance of  spectacular arch...   \n",
       "8     When you think of wave  surfing you probably h...   \n",
       "4108  From 1790 to 1880 the  United States  required...   \n",
       "2062  UPDATE: Currently closed by New  York departme...   \n",
       "\n",
       "                                               placeURL  \n",
       "1     https://www.atlasobscura.com/places/the-snow-h...  \n",
       "2049  https://www.atlasobscura.com/places/webster-place  \n",
       "8      https://www.atlasobscura.com/places/eisbachwelle  \n",
       "4108  https://www.atlasobscura.com/places/rothschild...  \n",
       "2062  https://www.atlasobscura.com/places/atlantic-a...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de6398-4808-4d92-a58a-2835a817523a",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1f1f53-60f2-44de-9c8d-7b9b3791e26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"vocabulary.json\", \"r\") as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "reverse_index_v2 = {}\n",
    "\n",
    "for w in s:\n",
    "    filtered = df_res.cleanedDesc.apply(lambda x: x != None and w in x)     # filter the documents that cointain the word\n",
    "    docs = df_res[filtered].cleanedDesc.apply(lambda x: x.count(w)/len(x) * np.log(len(df_res)/len(df_res[filtered])))  # compute the idf only for the files that contains the word\n",
    "    tfidf= list(zip(docs.index.tolist(), docs.values.tolist()))\n",
    "\n",
    "    #tfidf.append(list(zip(df_res[-filtered].index.tolist(), [0 for i in range(len(df_res[-filtered].index.tolist()))])))    # 0 for all the files that do not contain the word\n",
    "    \n",
    "    reverse_index_v2[vocabulary[w]] = tfidf\n",
    "    \n",
    "with open(\"reverse_index_v2.json\", \"w\") as f:\n",
    "    f.write(json.dumps(reverse_index_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e3e3e91-14de-47a2-874e-08314c711b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "def query_function_v2(query, voc=\"vocabulary.json\", reverse_index=\"reverse_index_v2.json\", df_name=\"res.tsv\"):\n",
    "    df = pd.read_csv(df_name, delimiter = '\\t')\n",
    "    with open(reverse_index, \"r\") as f:\n",
    "        reverse_index = json.load(f)\n",
    "    with open(voc, \"r\") as f:\n",
    "        vocabulary = json.load(f)\n",
    "    stemmed_query = text_cleaner_2(query)\n",
    "    token_query = list(map(lambda w: vocabulary[w],stemmed_query ))\n",
    "    \n",
    "    filtered_dict = { token: reverse_index[str(token)] for token in token_query }\n",
    "\n",
    "    tfidf_vec = list(map(lambda w: token_query.count(w)/len(token_query) * np.log(len(df)/len(filtered_dict[w])), token_query))\n",
    "    \n",
    "    \n",
    "    docs = set([tup[0] for l in filtered_dict.values() for tup in l ])\n",
    "    \n",
    "    \n",
    "    tfidf_docs = {}\n",
    "    for doc in docs:\n",
    "        v=[]\n",
    "        for word in token_query:\n",
    "            for t in reverse_index[str(word)]:\n",
    "                if t[0]==doc:\n",
    "                    v.append(t[1])\n",
    "                    break\n",
    "            else:\n",
    "                v.append(0)\n",
    "        tfidf_docs[doc] = v\n",
    "    \n",
    "    res = df.filter(items=list(docs), axis=0)\n",
    "    res[\"Cosine similarity\"] = res.index.map(lambda x: cosine_similarity(tfidf_vec,tfidf_docs[x] ))\n",
    "    \n",
    "    return res[[\"placeName\",\"placeDesc\",\"placeURL\",\"Cosine similarity\"]].sort_values(\"Cosine similarity\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c0e5da35-fdf6-4bfe-b471-23d1eb435094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeURL</th>\n",
       "      <th>Cosine similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>Grand Hotel Campo dei Fiori</td>\n",
       "      <td>With all the charm of  the Grand Budapest Hote...</td>\n",
       "      <td>https://www.atlasobscura.com/places/grand-hote...</td>\n",
       "      <td>0.774663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>Campo de Fiori</td>\n",
       "      <td>Flower stalls, cafes, and throngs  of tourists...</td>\n",
       "      <td>https://www.atlasobscura.com/places/campo-de-f...</td>\n",
       "      <td>0.774663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>Servian Wall at McDonald's</td>\n",
       "      <td>McDonald’s may be one of  the last places you’...</td>\n",
       "      <td>https://www.atlasobscura.com/places/servian-wa...</td>\n",
       "      <td>0.632375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6822</th>\n",
       "      <td>Stolpersteine Holocaust Memorials</td>\n",
       "      <td>All over Berlin and other  parts of Europe, ar...</td>\n",
       "      <td>https://www.atlasobscura.com/places/stolperste...</td>\n",
       "      <td>0.632375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>Relics of Zlătari Church</td>\n",
       "      <td>At a small church in  Bucharest ,  Romania , a...</td>\n",
       "      <td>https://www.atlasobscura.com/places/relics-of-...</td>\n",
       "      <td>0.632375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              placeName  \\\n",
       "4248        Grand Hotel Campo dei Fiori   \n",
       "437                      Campo de Fiori   \n",
       "3395         Servian Wall at McDonald's   \n",
       "6822  Stolpersteine Holocaust Memorials   \n",
       "4600          Relics of Zlătari Church    \n",
       "\n",
       "                                              placeDesc  \\\n",
       "4248  With all the charm of  the Grand Budapest Hote...   \n",
       "437   Flower stalls, cafes, and throngs  of tourists...   \n",
       "3395  McDonald’s may be one of  the last places you’...   \n",
       "6822  All over Berlin and other  parts of Europe, ar...   \n",
       "4600  At a small church in  Bucharest ,  Romania , a...   \n",
       "\n",
       "                                               placeURL  Cosine similarity  \n",
       "4248  https://www.atlasobscura.com/places/grand-hote...           0.774663  \n",
       "437   https://www.atlasobscura.com/places/campo-de-f...           0.774663  \n",
       "3395  https://www.atlasobscura.com/places/servian-wa...           0.632375  \n",
       "6822  https://www.atlasobscura.com/places/stolperste...           0.632375  \n",
       "4600  https://www.atlasobscura.com/places/relics-of-...           0.632375  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_function_v2(\"fiori Roma\").head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
