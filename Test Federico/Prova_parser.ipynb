{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31be67a6-630c-482e-a80f-082b3d46fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import bs4 as bs\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668e21c6-b808-4f68-a141-5331c74503c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from re import search\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3796eebd-c09c-400f-97d7-f00de8ac9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_place(page):\n",
    "    \n",
    "    # Read local files\n",
    "    with open(page) as f:\n",
    "        soup = bs.BeautifulSoup(f)\n",
    "    \n",
    "    placeName = soup.find_all('h1', {'class':'DDPage__header-title'})[0].contents[0]\n",
    "    \n",
    "    placeTags = list()\n",
    "    for tags in soup.find_all('a', {'class':'itemTags__link js-item-tags-link'}):\n",
    "        wordlen=len(tags.text)-2\n",
    "        tag = tags.text[1:wordlen]\n",
    "        placeTags.append(str(tag))\n",
    "    \n",
    "    been = soup.find_all('div', {'class':'col-xs-4X js-submit-wrap js-been-to-top-wrap action-btn-col hidden-print'})[0]\n",
    "    num_been = been.get_text().split()\n",
    "    numPeopleVisited = int(num_been[2])\n",
    "    if numPeopleVisited==0:\n",
    "        numPeopleVisited = ''\n",
    "    \n",
    "    want = soup.find_all('div', {'class':'col-xs-4X js-submit-wrap js-like-top-wrap action-btn-col hidden-print'})[0]\n",
    "    num_want = want.get_text().split()\n",
    "    numPeopleWant = int(num_want[3])\n",
    "    if numPeopleWant==0:\n",
    "        numPeopleWant = ''\n",
    "    \n",
    "    description = soup.find('div', class_='DDP__body-copy')\n",
    "    allowlist = ['p', 'span', 'a', 'i']\n",
    "    text_elements = [t for t in description.find_all(text=True) if t.parent.name in allowlist]\n",
    "    placeDesc = str(' '.join(text_elements))\n",
    "    placeDesc = placeDesc.replace(u'\\xa0',u' ')\n",
    "    \n",
    "    \n",
    "    placeShortDesc = soup.find_all('h3', {'class':'DDPage__header-dek'})[0].contents[0]\n",
    "    placeShortDesc = placeShortDesc.replace(u'\\xa0',u' ')\n",
    "    placeShortDesc = str(placeShortDesc)\n",
    "    \n",
    "    placeNearby=list()\n",
    "    for places in soup.find_all('div', {'class':'DDPageSiderailRecirc__item-title'}):\n",
    "        placeNearby.append(str(places.text))\n",
    "    if len(placeNearby) == 0:\n",
    "        placeNearby = ''\n",
    "    \n",
    "    \n",
    "    placeRaw= soup.find_all('address', class_='DDPageSiderail__address')[0]\n",
    "    place = placeRaw.find_all('div')[0].contents[0:5:2]\n",
    "    place = \" \".join(place)\n",
    "    placeAddress = place.replace('\\n', '')\n",
    "    \n",
    "    \n",
    "    coordinates = soup.find_all('div', class_='DDPageSiderail__coordinates')[0]\n",
    "    coordinates = coordinates.get_text().split()\n",
    "    Alt = coordinates[0]\n",
    "    Altlen = len(Alt)\n",
    "    placeAlt = float(Alt[0:Altlen-1])\n",
    "    placeLong = float(coordinates[1])\n",
    "    \n",
    "\n",
    "    editors = soup.find_all('li', {'class':'DDPContributorsList__item'})\n",
    "    if len(editors)==0:\n",
    "        #placeEditors = soup.find_all('div', {'class':'DDPContributorsList'})[1].get_text().split()\n",
    "        #TODO: check the line below\n",
    "        listEditor = soup.find_all('div', {'class':'DDPContributorsList'})\n",
    "        if len(listEditor) == 0:\n",
    "            placeEditors=[\"\"]\n",
    "        else:\n",
    "            placeEditors = listEditor[0].get_text().split()\n",
    "    else:\n",
    "        placeEditors = list()\n",
    "        for place in editors:\n",
    "            names = place.find('span').getText()\n",
    "            placeEditors.append(names)\n",
    "    \n",
    "    \n",
    "    date_time = soup.find_all('div', {'class':'DDPContributor__name'})[0].get_text()\n",
    "    placePubDate = datetime.strptime(date_time, '%B %d, %Y')\n",
    "    \n",
    "    \n",
    "    titles = soup.find_all('h3', class_='Card__heading --content-card-v2-title js-title-content')  \n",
    "    placeRelatedPlaces = list()\n",
    "    for title in titles:\n",
    "        big_check = title.parent.parent.parent.parent.parent.parent\n",
    "        check = big_check.find('div', class_=\"CardRecircSection__title\").get_text()\n",
    "        if check == 'Related Places':\n",
    "            placeRelatedPlaces.append(str(title.get_text().strip()))\n",
    "    \n",
    "    placeRelatedLists = list()\n",
    "    for title in titles:\n",
    "        big_check = title.parent.parent.parent.parent.parent.parent\n",
    "        check = big_check.find('div', class_=\"CardRecircSection__title\").get_text()\n",
    "        if search(\"Appears in\", check):\n",
    "            placeRelatedLists.append(str(title.get_text().strip()))\n",
    "    if len(placeRelatedLists)==0:\n",
    "        placeRelatedLists.append('')\n",
    "    \n",
    "    find_url = soup.find('link', {\"rel\": \"canonical\"})\n",
    "    placeURL = find_url['href']\n",
    "    \n",
    "    #print(\"placeName: \"+str(len(placeName)))\n",
    "    #print(\"placetags \"+str(len(placeTags)))\n",
    "    #print(\"address \"+str(len(placeAddress)))\n",
    "    #print(\"editors \"+str(len(placeEditors)))\n",
    "    #print(\"relatedplaces \"+str(len(placeRelatedPlaces)))\n",
    "    #print(\"relatedlists \"+str(len(placeRelatedLists)))\n",
    "\n",
    "    \n",
    "    return {'placeName': placeName,\n",
    "            'placeTags': str(placeTags),\n",
    "            'numPeopleVisited': numPeopleVisited,\n",
    "            'numPeopleWant': numPeopleWant,\n",
    "            'placeDesc': placeDesc,\n",
    "            'placeShortDesc':placeShortDesc,\n",
    "            'placeNearby':str(placeNearby),\n",
    "            'placeAddress': placeAddress,\n",
    "            'placeAlt': placeAlt,\n",
    "            'placeLong': placeLong,\n",
    "            'placeEditors': str(placeEditors),\n",
    "            'placePubDate': placePubDate,\n",
    "            'placeRelatedPlaces': str(placeRelatedPlaces),\n",
    "            'placeRelatedLists': str(placeRelatedLists),\n",
    "            'placeURL': placeURL}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88878e73-f876-4351-8975-c4768dfa0d2e",
   "metadata": {},
   "source": [
    "reference:\\\n",
    "https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba0ce34-0d42-4c6c-9ee2-e335cd7234cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_maker(pages:list ,dir=\"downloads\"):\n",
    "    raws = []\n",
    "    \n",
    "    for page in pages:\n",
    "        df = pd.DataFrame(extract_single_place(f\"{dir}/{page}\"), index=[0])\n",
    "        raws.append(df)\n",
    "    return pd.concat(raws)\n",
    "\n",
    "def parallel_table(dir=\"downloads\"):\n",
    "    n_cores = cpu_count()\n",
    "    files = os.listdir(dir)\n",
    "    files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "    chunks = np.split(np.array(files), n_cores)\n",
    "    \n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(table_maker, chunks))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c6f04-d689-4cae-ae3b-d9e72968d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_pages = parallel_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42bf37-a9f4-44ac-8962-3c1f28c81cc7",
   "metadata": {},
   "source": [
    "### Debugging di extract_single_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91f813-5629-449e-9169-e3dcfa54c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "files = os.listdir(\"downloads/\")\n",
    "files.remove(\".ipynb_checkpoints\")   # remove this junk\n",
    "for file in files:\n",
    "    try:\n",
    "        extract_single_place(f\"downloads/{file}\")\n",
    "    except Exception as e:\n",
    "        print(file,\"-->\" ,traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
